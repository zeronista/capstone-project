# üéôÔ∏è ASR Service - Whisper Large V3

Self-hosted Speech-to-Text service s·ª≠ d·ª•ng OpenAI Whisper Large V3 model.

## üöÄ Quick Start

### 1. Ch·∫°y v·ªõi Docker (GPU)

```bash
# Build v√† ch·∫°y v·ªõi GPU support
docker compose up -d asr-gpu

# Ki·ªÉm tra logs
docker logs -f asr-service-gpu
```

### 2. Ch·∫°y v·ªõi Docker (CPU only)

```bash
# Build v√† ch·∫°y CPU version
docker compose --profile cpu up -d asr-cpu

# L∆∞u √Ω: CPU version ch·∫≠m h∆°n nhi·ªÅu (~10-20x)
```

### 3. Development mode (model nh·ªè)

```bash
# D√πng model base ƒë·ªÉ test nhanh
docker compose --profile dev up -d asr-dev
```

## üì° API Endpoints

### Transcribe Audio

```bash
# Full response v·ªõi timestamps
curl -X POST "http://localhost:8001/transcribe" \
     -F "file=@audio.wav" \
     -F "language=vi"

# Simple response (ch·ªâ text)
curl -X POST "http://localhost:8001/transcribe/simple" \
     -F "file=@audio.wav"
```

### Response Format

```json
{
  "success": true,
  "text": "Xin ch√†o, t√¥i l√† b·ªánh nh√¢n Nguy·ªÖn VƒÉn A",
  "language": "vi",
  "language_probability": 0.9876,
  "duration": 5.432,
  "segments": [
    {
      "id": 0,
      "start": 0.0,
      "end": 2.5,
      "text": "Xin ch√†o",
      "confidence": -0.234
    },
    {
      "id": 1,
      "start": 2.5,
      "end": 5.432,
      "text": "t√¥i l√† b·ªánh nh√¢n Nguy·ªÖn VƒÉn A",
      "confidence": -0.156
    }
  ],
  "processing_time": 1.234
}
```

### Health Check

```bash
curl http://localhost:8001/health
```

## ‚öôÔ∏è Configuration

| Variable | Default | Description |
|----------|---------|-------------|
| `MODEL_SIZE` | `large-v3` | Model size: tiny, base, small, medium, large-v3 |
| `DEVICE` | `auto` | Device: auto, cuda, cpu |
| `COMPUTE_TYPE` | `float16` | GPU: float16, int8_float16. CPU: int8 |
| `BEAM_SIZE` | `5` | Beam search size (1-10) |
| `MAX_FILE_SIZE_MB` | `100` | Max upload file size |

## üìä Model Sizes & Performance

| Model | Parameters | VRAM | Speed (GPU) | Speed (CPU) | Quality |
|-------|------------|------|-------------|-------------|---------|
| tiny | 39M | ~1GB | ~32x | ~1x | ‚≠ê‚≠ê |
| base | 74M | ~1GB | ~16x | ~0.7x | ‚≠ê‚≠ê‚≠ê |
| small | 244M | ~2GB | ~6x | ~0.3x | ‚≠ê‚≠ê‚≠ê‚≠ê |
| medium | 769M | ~5GB | ~2x | ~0.1x | ‚≠ê‚≠ê‚≠ê‚≠ê |
| large-v3 | 1550M | ~10GB | ~1x | ~0.05x | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |

*Speed relative to real-time audio length*

## üîó Integration v·ªõi Spring Boot

### C·∫≠p nh·∫≠t application.properties

```properties
# ASR Service (self-hosted Whisper)
asr.service.url=http://localhost:8001
asr.service.enabled=true
```

### Java Client Example

```java
@Service
public class WhisperASRService {
    
    @Value("${asr.service.url}")
    private String asrServiceUrl;
    
    public String transcribe(MultipartFile audioFile) {
        RestTemplate restTemplate = new RestTemplate();
        
        HttpHeaders headers = new HttpHeaders();
        headers.setContentType(MediaType.MULTIPART_FORM_DATA);
        
        MultiValueMap<String, Object> body = new LinkedMultiValueMap<>();
        body.add("file", new ByteArrayResource(audioFile.getBytes()) {
            @Override
            public String getFilename() {
                return audioFile.getOriginalFilename();
            }
        });
        body.add("language", "vi");
        
        HttpEntity<MultiValueMap<String, Object>> requestEntity = 
            new HttpEntity<>(body, headers);
        
        ResponseEntity<Map> response = restTemplate.postForEntity(
            asrServiceUrl + "/transcribe/simple",
            requestEntity,
            Map.class
        );
        
        return (String) response.getBody().get("text");
    }
}
```

## üê≥ Docker Commands

```bash
# Build images
docker compose build

# Start GPU service
docker compose up -d asr-gpu

# Start CPU service
docker compose --profile cpu up -d asr-cpu

# View logs
docker logs -f asr-service-gpu

# Stop all
docker compose down

# Remove volumes (x√≥a model cache)
docker compose down -v
```

## üìÅ Folder Structure

```
asr-service/
‚îú‚îÄ‚îÄ Dockerfile           # Multi-stage: GPU & CPU variants
‚îú‚îÄ‚îÄ docker-compose.yml   # Service definitions
‚îú‚îÄ‚îÄ main.py              # FastAPI application
‚îú‚îÄ‚îÄ requirements.txt     # GPU dependencies
‚îú‚îÄ‚îÄ requirements-cpu.txt # CPU dependencies
‚îú‚îÄ‚îÄ .env.example         # Environment template
‚îî‚îÄ‚îÄ README.md            # This file
```

## üîß Troubleshooting

### Model download ch·∫≠m
```bash
# Pre-download model tr∆∞·ªõc khi ch·∫°y
docker run --rm -v asr-whisper-models:/models \
  python:3.11-slim \
  pip install faster-whisper && \
  python -c "from faster_whisper import WhisperModel; WhisperModel('large-v3', download_root='/models')"
```

### Out of Memory (GPU)
```bash
# D√πng compute_type ti·∫øt ki·ªám VRAM h∆°n
COMPUTE_TYPE=int8_float16

# Ho·∫∑c d√πng model nh·ªè h∆°n
MODEL_SIZE=medium
```

### CPU qu√° ch·∫≠m
```bash
# D√πng model nh·ªè h∆°n cho CPU
MODEL_SIZE=small
COMPUTE_TYPE=int8
NUM_WORKERS=4
```

## üìù License

MIT License - S·ª≠ d·ª•ng cho m·ª•c ƒë√≠ch h·ªçc t·∫≠p v√† nghi√™n c·ª©u.
